{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1 - Programmation Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alexis/.local/lib/python3.10/site-packages/ipykernel_launcher.py\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.argv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NB: N'hésitez pas à réutiliser des fonctions d'une partie à l'autre !\n",
    "\n",
    "## Analyse de texte : listes, dictionnaires, ensembles\n",
    "\n",
    "Afin de gagner en familiarité avec les structures de données Python les plus utiles, vous allez\n",
    "développer des fonctions pour analyser le vocabulaire utilisé dans un texte. \n",
    "Vous trouverez sur ma page une version d'Alice aux pays des merveilles, mais vous pouvez récupérer le texte\n",
    "de votre choix. J'ai choisi un texte an anglais pour simplifier les problèmes d'accents. \n",
    "\n",
    "http://www.irit.fr/~Philippe.Muller/alice_wonderland.utf8.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Première approche simple\n",
    "\n",
    "Définissez un ensemble de fonctions pour lire un texte d'un fichier, et compter le nombre d'occurrence de chaque mot.  \n",
    "Il faudra bien sûr gérer la ponctuation, entre autres.  \n",
    "Le but est de sortir les mots les plus significatifs utilisés dans l'oeuvre considérée.  \n",
    "Quels sont les mots les plus fréquents dans le texte ? Qu'en pensez vous ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "[(398, 'alice'), (144, 'very'), (128, 'little'), (117, 'out'), (102, 'down'), (99, 'up'), (83, 'again'), (75, 'queen'), (71, 'time'), (63, 'king'), (58, 'turtle'), (56, 'way'), (56, 'mock'), (56, 'hatter'), (55, 'quite'), (55, 'gryphon'), (53, 'think'), (51, 'rabbit'), (51, 'here'), (51, 'first'), (50, 'only'), (50, 'much'), (50, 'head'), (50, 'go'), (49, 'which'), (49, 'thing'), (49, 'more'), (48, 'voice'), (47, 'never'), (46, '\"'), (45, 'looked'), (45, 'got'), (45, 'get'), (44, 've'), (44, 'oh'), (44, 'must'), (44, 'mouse'), (44, 'come'), (43, 'him'), (43, 'after'), (42, 'duchess'), (41, 'round'), (40, 'why'), (40, 'two'), (40, 'tone'), (40, 'such'), (40, 'over'), (40, 'other'), (40, 'dormouse'), (40, 'came')]\n"
     ]
    }
   ],
   "source": [
    "class TextTokens:\n",
    "\n",
    "    # Attributes\n",
    "    ponct = set(\",;?.:-!&'()\")\n",
    "    stop_words = set(\"one the a some no not is be are has and to it she i of said you in that as her t at s on all with had but for they so vey what he there if his then this them were would was herself do have when or could went off me into see how well m did went about know like can your who don now * my by began ll its thought an their just say\".split())  # Split stop keywords to remove from the text\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.file = open(filename).read().strip()\n",
    "\n",
    "    @staticmethod\n",
    "    def comptage(liste_items):\n",
    "        res = {}\n",
    "        for i in liste_items:\n",
    "            res[i] = res.get(i, 0) + 1\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def normalise(mot):\n",
    "        res = mot.strip().replace(\"'\", \"\").lower()\n",
    "        return res\n",
    "\n",
    "    def enleve_ponctuation(self, text):\n",
    "        res = text\n",
    "        for sign in self.ponct:\n",
    "            res = res.replace(sign, \" \")\n",
    "        return res\n",
    "\n",
    "    def list_tokens(self, text):\n",
    "        new_text = self.enleve_ponctuation(text)\n",
    "        mots = [self.normalise(mot) for mot in new_text.split() if self.normalise(mot) not in self.stop_words]\n",
    "        return mots\n",
    "\n",
    "    def stats_text(self, text):\n",
    "        mots = self.list_tokens(text)\n",
    "        return self.comptage(mots)\n",
    "\n",
    "    def sorted_stat(self):\n",
    "        stats = self.stats_text(self.file)\n",
    "        sorted_stats = [(y, x) for (x, y) in list(stats.items())]\n",
    "        sorted_stats.sort()\n",
    "        sorted_stats.reverse()\n",
    "        return sorted_stats\n",
    "\n",
    "    def display(self, sorted_stats):\n",
    "        print(\"Tokens:\")\n",
    "        print(sorted_stats[:50])\n",
    "\n",
    "doc = TextTokens(\"alice_wonderland_utf8.txt\")\n",
    "doc.display(doc.sorted_stat())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texte prétraité\n",
    "\n",
    "Vous avez du remarquer entre autres problèmes, que certains mots que l'on voudrait regrouper apparaissent sous des formes différentes (pluriel des noms, verbes conjugués), et que les mots fonctionnels (déterminants, prépositions par exemple) sont courants sans être très intéressants.  \n",
    "Vous trouverez dans le fichier [alice_wonderland.utf8.conll](http://www.irit.fr/~Philippe.Muller/alice_wonderland.utf8.conll) une version du texte déjà prétraité, où chaque ligne correspond à une analyse prélable d'un mot du texte, avec sa forme telle qu'elle apparait dans le texte, son lemme (cad la forme normalisée correspondant à son entrée dans le dictionnaire), et une étiquette donnant sa catégorie: nom, verbe, déterminant, etc.  \n",
    "\n",
    "**Ecrivez de nouvelles fonctions pour refaire les analyses précédentes de façon plus simple avec ce fichier, en essayant de paramétrer le plus possible (faire varier les catégories à garder par exemple).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (Top 50):\n",
      "little: 111\n",
      "other: 52\n",
      "great: 39\n",
      "large: 33\n",
      "first: 33\n",
      "last: 31\n",
      "electronic: 27\n",
      "next: 26\n",
      "poor: 25\n",
      "long: 25\n",
      "same: 24\n",
      "good: 24\n",
      "old: 19\n",
      "curious: 19\n",
      "such: 18\n",
      "right: 18\n",
      "much: 18\n",
      "sure: 16\n",
      "full: 16\n",
      "low: 14\n",
      "many: 13\n",
      "mad: 13\n",
      "high: 13\n",
      "whole: 12\n",
      "small: 12\n",
      "afraid: 12\n",
      "glad: 11\n",
      "own: 10\n",
      "few: 10\n",
      "different: 10\n",
      "'Of: 10\n",
      "white: 8\n",
      "ready: 8\n",
      "public: 8\n",
      "free: 8\n",
      "beautiful: 8\n",
      "silent: 7\n",
      "new: 7\n",
      "moral: 7\n",
      "golden: 7\n",
      "deep: 7\n",
      "bright: 7\n",
      "'The: 7\n",
      "sharp: 6\n",
      "second: 6\n",
      "nice: 6\n",
      "melancholy: 6\n",
      "http:/: 6\n",
      "young: 5\n",
      "wrong: 5\n"
     ]
    }
   ],
   "source": [
    "class PreprocessedTextTokens:\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.file = open(filename, 'r', encoding='utf-8').readlines()\n",
    "\n",
    "    def list_tokens(self, categories_to_keep=None):\n",
    "        tokens = []\n",
    "        for line in self.file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 3:\n",
    "                word, category, lemma = parts\n",
    "                if categories_to_keep is None or category in categories_to_keep:\n",
    "                    tokens.append((word, lemma))\n",
    "        return tokens\n",
    "\n",
    "    def count_tokens(self, tokens):\n",
    "        token_count = {}\n",
    "        for token, lemma in tokens:\n",
    "            token_count[token] = token_count.get(token, 0) + 1\n",
    "        return token_count\n",
    "\n",
    "    def sorted_token_stat(self, categories_to_keep=None):\n",
    "        tokens = self.list_tokens(categories_to_keep)\n",
    "        token_count = self.count_tokens(tokens)\n",
    "        sorted_stats = [(count, token) for token, count in token_count.items()]\n",
    "        sorted_stats.sort(reverse=True)\n",
    "        return sorted_stats\n",
    "\n",
    "    def display(self, sorted_stats, n=50):\n",
    "        print(f\"Tokens (Top {n}):\")\n",
    "        for count, token in sorted_stats[:n]:\n",
    "            print(f\"{token}: {count}\")\n",
    "\n",
    "doc = PreprocessedTextTokens(\"alice_wonderland.utf8.conll\")\n",
    "\n",
    "# Définir les catégories grammaticales à garder (par exemple, NNP pour les noms propres)\n",
    "categories_to_keep = [\"JJ\"]\n",
    "\n",
    "sorted_stats = doc.sorted_token_stat(categories_to_keep)\n",
    "doc.display(sorted_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse de séquences\n",
    "Pour avoir des informations plus intéressantes, on peut aussi regarder les séquences de 2 mots consécutifs.  \n",
    "Ecrivez des fonctions pour compter toutes les séquences avec l'approche simple, et garder les plus \"intéressantes\"  \n",
    "Généraliser pour compter des séquences de longueur arbitraire (fixée à l'avance). On appelle ces séquences de n mots des n-grammes (bigrammes pour n=2, trigrammes pour n=3, etc).  \n",
    "\n",
    "Vous pouvez aller voir par curiosité l'inventaire historique fait par Google https://books.google.com/ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /home/alexis/.local/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.9/773.9 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2023.10.3 tqdm-4.66.1\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alexis/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-grams (Top 50):\n",
      "(',', 'and'): 450\n",
      "(',', \"'\"): 429\n",
      "(\"'\", 'said'): 329\n",
      "('!', \"'\"): 283\n",
      "('.', \"'\"): 262\n",
      "('said', 'the'): 206\n",
      "(\"'\", 'I'): 169\n",
      "('?', \"'\"): 157\n",
      "('of', 'the'): 127\n",
      "('said', 'Alice'): 115\n",
      "('in', 'a'): 95\n",
      "(',', 'I'): 81\n",
      "(\"'\", 'the'): 81\n",
      "('Alice', ','): 78\n",
      "('in', 'the'): 76\n",
      "('and', 'the'): 72\n",
      "('to', 'the'): 69\n",
      "('it', 'was'): 62\n",
      "('the', 'Queen'): 62\n",
      "(',', 'as'): 61\n",
      "(',', 'but'): 60\n",
      "('at', 'the'): 60\n",
      "('it', ','): 57\n",
      "('*', '*'): 57\n",
      "('as', 'she'): 56\n",
      "('a', 'little'): 56\n",
      "(\"'\", 'Alice'): 56\n",
      "('she', 'had'): 55\n",
      "('the', 'King'): 55\n",
      "('Mock', 'Turtle'): 55\n",
      "('I', \"'m\"): 54\n",
      "('Alice', '.'): 54\n",
      "('--', \"'\"): 52\n",
      "(';', 'and'): 52\n",
      "(',', 'you'): 51\n",
      "('she', 'was'): 50\n",
      "(',', 'she'): 50\n",
      "('.', 'The'): 50\n",
      "('to', 'be'): 50\n",
      "(\"'\", \"'\"): 50\n",
      "('the', 'Gryphon'): 50\n",
      "('the', 'Mock'): 49\n",
      "('went', 'on'): 48\n",
      "('.', 'Alice'): 47\n",
      "(\"'\", 'she'): 46\n",
      "('do', \"n't\"): 46\n",
      "('the', 'Hatter'): 46\n",
      "('to', 'herself'): 45\n",
      "(',', 'that'): 45\n",
      "('and', 'she'): 43\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "nltk.download(\"punkt\")  # Télécharger les données nécessaires\n",
    "\n",
    "class NgramAnalyzer:\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.file = open(filename, 'r', encoding='utf-8').read()\n",
    "\n",
    "    def count_ngrams(self, n):\n",
    "        tokens = nltk.word_tokenize(self.file)\n",
    "        n_grams = list(ngrams(tokens, n))\n",
    "        fdist = FreqDist(n_grams)\n",
    "        return fdist\n",
    "\n",
    "    def sorted_ngram_stat(self, n, top_n=50):\n",
    "        fdist = self.count_ngrams(n)\n",
    "        return fdist.most_common(top_n)\n",
    "\n",
    "    def display(self, sorted_stats):\n",
    "        print(f\"{n}-grams (Top {len(sorted_stats)}):\")\n",
    "        for ngram, count in sorted_stats:\n",
    "            print(f\"{ngram}: {count}\")\n",
    "\n",
    "doc = NgramAnalyzer(\"alice_wonderland_utf8.txt\")\n",
    "\n",
    "# Spécifiez la longueur de l'analyse des n-grammes (n)\n",
    "n = 2  # Pour des bigrammes, par exemple\n",
    "\n",
    "sorted_stats = doc.sorted_ngram_stat(n)\n",
    "doc.display(sorted_stats)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
